# CS5760 ‚Äì Homework 5  
**Natural Language Processing ‚Äì Fall 2025**  
**University of Central Missouri**  
**Department of Data Science &Ai**  

**Student Name:** Duggineni Sesha Rao  

---

## üìå Assignment Description
This assignment covers **AI Ethics, Bias, Safety, Privacy, and Harm Mitigation**.  
Part I includes **short answer questions** where each response explains key concepts in responsible AI development.

This README explains **what each question asked** and **what was answered** in the submitted PDF.

---

## üìù Question-by-Question Explanation

### **Q1. Ethical Foundations**
This question focuses on the difference between ethics and personal/legal rules, and key ethical theories:

- **Part (a)**  
  I explained that **ethics is not the same as laws or feelings**:  
  - Laws are government rules and may still support unethical outcomes  
  - Feelings are personal and unreliable  
  Ethics is based on reasoning about right vs. wrong.

- **Part (b)**  
  I described **Utilitarianism** (outcome-based) and **Deontology** (rule-based) and how each would guide AI decisions (e.g., privacy vs. benefit).

- **Part (c)**  
  I explained that **no single ethical theory solves all problems**, because different situations involve conflicting values like fairness, safety, and rights.

---

### **Q2. Types of AI Harms**
This question addresses two major harms AI systems can cause:

- **Part (a)**  
  Defined:  
  - **Allocational harm** ‚Üí unfair denial of opportunities/resources  
  - **Representational harm** ‚Üí disrespect, stereotypes, biased portrayal

- **Part (b)**  
  Provided real examples:  
  - Hiring algorithm unfair to women (allocational)  
  - Translation model reinforcing gender roles (representational)

- **Part (c)**  
  I explained that **representational harm is harder to measure**, because it affects culture and dignity, not just numbers.

---

### **Q3. Sources of Dataset Bias**
This question explains where data bias originates:

- **Part (a)**  
  I identified three sources:  
  - **Sampling bias** (data from limited population)  
  - **Historical bias** (societal inequality reflected in data)  
  - **Annotation bias** (labelers‚Äô assumptions)

- **Part (b)**  
  Under-represented groups listed:  
  - Low-resource languages  
  - Minority communities  
  - Disabled and older populations

- **Part (c)**  
  I explained **bias amplification**, where models exaggerate existing bias during training ‚Äî even after preprocessing.

---

### **Q4. Safety, Security & Privacy**
This question focuses on adversarial threats in AI development:

- **Part (a)**  
  I defined **data poisoning**:  
  Attackers inject harmful data to manipulate outcomes (e.g., mislabeling stop signs).

- **Part (b)**  
  I described ethical issues of **model memorization**:  
  - Leaks personal data  
  - Reproduces copyrighted text

- **Part (c)**  
  I explained how **model stealing** threatens privacy and intellectual property by copying a model‚Äôs functionality through repeated queries.

---

### **Q5. Harm Mitigation & Best Practices**
This question covers solutions to ethical issues:

- **Part (a)**  
  Mitigation approaches given:  
  - Technical ‚Üí fairness-aware training methods  
  - Non-technical ‚Üí stakeholder review & impact assessment

- **Part (b)**  
  I stated that **Model Cards** and **Bias Audits** improve **transparency and accountability** by documenting model risks and fairness.

- **Part (c)**  
  Explained **expanding the ethical circle**:  
  Including marginalized groups in decision-making ensures **inclusive AI development** that benefits everyone.

---



---
